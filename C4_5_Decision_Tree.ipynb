{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assessment 2: C4.5 Decision Tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXOwIpZWFNH"
      },
      "source": [
        "# **The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSRZxz4Z7vWc",
        "outputId": "fc840a37-6073-4c8b-9a83-7f016e8fdb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "\n",
        "## The dataset is sourced from Kaggle and imported into my personal Github Repository\n",
        "## The dataset contains COVID-19 information related to cases, deaths, recoveries, etc.\n",
        "url = 'https://raw.githubusercontent.com/AndrewDucDanhDo/machine_learning_assessment_2_dataset/main/country_wise_latest.csv'\n",
        "raw_data = pd.read_csv(url)\n",
        "\n",
        "\n",
        "attributes = ['Confirmed','Deaths', 'Recovered', 'Active', 'New cases', 'New deaths', 'New recovered']\n",
        "target = \"Habitability\"\n",
        "\n",
        "# Divide the dataset into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(raw_data[attributes], raw_data[target], test_size=0.3, shuffle=False)\n",
        "\n",
        "# Robust Scaler will transform the dataset in consideration of the extreme outliers (Ex. US COVID-19 data is highest in the world)\n",
        "X_train_robust_scaler = RobustScaler(quantile_range=(25, 75)).fit_transform(X_train)\n",
        "X_test_robust_scaler = RobustScaler(quantile_range=(25, 75)).fit_transform(X_test)\n",
        "\n",
        "# The minmax_scale scales the dataset between 0-1\n",
        "X_train_scale = minmax_scale(X_train_robust_scaler)\n",
        "X_test_scale = minmax_scale(X_test_robust_scaler)\n",
        "\n",
        "\n",
        "# The dataset is then binned so that the data is categorical instead of continuous\n",
        "enc = KBinsDiscretizer(n_bins=6, encode='ordinal')\n",
        "X_train_binned = enc.fit_transform(X_train_scale)\n",
        "\n",
        "enc = KBinsDiscretizer(n_bins=6, encode='ordinal')\n",
        "X_test_binned = enc.fit_transform(X_test_scale)\n",
        "\n",
        "# Convert the dataset back to a csv file for later usage\n",
        "X_train_processed = pd.DataFrame(X_train_binned, columns=[attributes])\n",
        "X_train_processed.to_csv('i.csv', index=False, header=False)\n",
        "\n",
        "X_test_processed = pd.DataFrame(X_test_binned, columns=[attributes])\n",
        "X_test_processed.to_csv('i.csv', index=False, header=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
            "  'decreasing the number of bins.' % jj)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ2iWgXbjMva"
      },
      "source": [
        "# Decides whether the binned dataset is Habitable for the training dataset to build the tree and for later comparison with the test dataset (answers)\n",
        "def check_habitable(dataset):\n",
        "    l = []\n",
        "    for i in  range(len(dataset)):\n",
        "        x = 0\n",
        "        for j in range(len(dataset[0])):       \n",
        "            if ((i == 0 or i == 1 or i == 3) and dataset[i][j]>3):\n",
        "                x +=1\n",
        "            elif (i == 2 and dataset[i][j] <= 3):\n",
        "                x += 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        if (x > 2):\n",
        "            l.append(\"No\")\n",
        "        else:\n",
        "            l.append(\"Yes\")\n",
        "    \n",
        "    return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN76bFTTBKN5",
        "outputId": "1870955b-3907-4a66-bb18-8017cb92718a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Train \"Habitabilty\" column dataset used to build the decision tree\n",
        "train_habitable = check_habitable(X_train_binned)\n",
        "y_train_processed = pd.DataFrame(train_habitable, columns=['Habitability'])\n",
        "y_train_processed.to_csv('i.csv', index=False, header=False)\n",
        "\n",
        "## Test \"Habitability\" column dataset used for answer comparison with the predicted\n",
        "test_habitable = check_habitable(X_test_binned)\n",
        "y_test_processed = pd.DataFrame(test_habitable, columns=['Habitability'])\n",
        "y_test_processed.to_csv('i.csv', index=False, header=False)\n",
        "\n",
        "print(y_test_processed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Habitability\n",
            "0           Yes\n",
            "1           Yes\n",
            "2           Yes\n",
            "3            No\n",
            "4           Yes\n",
            "5           Yes\n",
            "6           Yes\n",
            "7           Yes\n",
            "8           Yes\n",
            "9           Yes\n",
            "10          Yes\n",
            "11          Yes\n",
            "12          Yes\n",
            "13          Yes\n",
            "14          Yes\n",
            "15          Yes\n",
            "16          Yes\n",
            "17          Yes\n",
            "18          Yes\n",
            "19          Yes\n",
            "20          Yes\n",
            "21          Yes\n",
            "22          Yes\n",
            "23          Yes\n",
            "24          Yes\n",
            "25          Yes\n",
            "26          Yes\n",
            "27          Yes\n",
            "28          Yes\n",
            "29          Yes\n",
            "30          Yes\n",
            "31          Yes\n",
            "32          Yes\n",
            "33          Yes\n",
            "34          Yes\n",
            "35          Yes\n",
            "36          Yes\n",
            "37          Yes\n",
            "38          Yes\n",
            "39          Yes\n",
            "40          Yes\n",
            "41          Yes\n",
            "42          Yes\n",
            "43          Yes\n",
            "44          Yes\n",
            "45          Yes\n",
            "46          Yes\n",
            "47          Yes\n",
            "48          Yes\n",
            "49          Yes\n",
            "50          Yes\n",
            "51          Yes\n",
            "52          Yes\n",
            "53          Yes\n",
            "54          Yes\n",
            "55          Yes\n",
            "56          Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSFxQrGWnprA"
      },
      "source": [
        "# **The Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_6Vs2npW-Sl"
      },
      "source": [
        "def compute_entropy(sample):\n",
        "    ## :PARAM sample: The data sample of a discrete distribution\n",
        "    \n",
        "    ## If the sample is of size 1 or less, the sample is too small\n",
        "    if len(sample) < 2:\n",
        "        return 0\n",
        "\n",
        "    ## Create an array for each unique value's proportion in the sample\n",
        "    ## \"normalise=True\": creates a proportion of the unique counts on a scale of 0-1\n",
        "    freq = np.array( sample.value_counts(normalize=True))\n",
        "\n",
        "    ## Formula to calculate the entropy of the sample\n",
        "    ## NOTE 1e-6 prevents the log from causing an error\n",
        "    return -(freq * np.log2(freq + 1e-6)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qeKd-7K_nkd"
      },
      "source": [
        "def compute_info_gain(samples, key, target):\n",
        "  ### Infomration Gain: E(Y) - E(Y|X)\n",
        "\n",
        "    ## Get each proportion of unique values for the key attribute\n",
        "    values = samples[key].value_counts(normalize=True)\n",
        "    weighted_average_entropy = 0\n",
        "\n",
        "    for value, frequency in values.iteritems():\n",
        "        index = samples[key]==value\n",
        "        ## \"sub_entropy\": the entropy of one of the unique values of the sample\n",
        "        sub_entropy = compute_entropy(target[index])\n",
        "        ## \"weighted_average_entropy\": the summation of the weighted entropies\n",
        "        weighted_average_entropy += frequency * sub_entropy\n",
        "    \n",
        "    ## \"target_entropy\": the target attribute's total entropy\n",
        "    target_entropy = compute_entropy(target)\n",
        "\n",
        "    ## Return the information gain through the subtraction of entropies\n",
        "    return target_entropy - weighted_average_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPV3_WmcIe9J"
      },
      "source": [
        "class TreeNode:\n",
        "    \"\"\"\n",
        "    A recursively defined data structure to store a tree.\n",
        "    Each node can contain other nodes as its children\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        ## Holds the sub nodes for recursion\n",
        "        # Recursive structure, those elements of the same type (TreeNode)\n",
        "        self.children = {}\n",
        "\n",
        "        ## Starting out the decision is undecided\n",
        "        self.decision = None \n",
        "        ## Splitting feature\n",
        "        self.split_attribute = None\n",
        "\n",
        "    def fit(self, samples, target):\n",
        "        \"\"\"\n",
        "        The function accepts a training dataset, from which it builds the tree \n",
        "        structure to make decisions or to make children nodes (tree branches) \n",
        "        to do further inquiries\n",
        "        :param samples: [n * p] n observed data samples of p attributes\n",
        "        :param target: [n] target values\n",
        "        \"\"\"\n",
        "        ## If the sample is empty \\\n",
        "        ## the decision should be the node parent's decision\n",
        "        if len(samples) == 0:\n",
        "            self.decision = \"Yes\"\n",
        "            return\n",
        "\n",
        "        ## Otherwise, calculate for the maximum information gain\n",
        "        else:\n",
        "            ## If there is only one unique value in the target column \\\n",
        "            ## the decision should be set to that unique value\n",
        "            if len(target.unique()) == 1:\n",
        "                self.decision = target.unique()[0]\n",
        "                return\n",
        "            else:    \n",
        "                max_info_gain = 0\n",
        "                ## Loop through the sample columns\n",
        "                for key in samples.keys():\n",
        "                    ## Call the function to compute the information gain \\\n",
        "                    ## for the current key\n",
        "                    key_info_gain = compute_info_gain(samples, key, target)\n",
        "\n",
        "                    ## If the current key's information gain is greater \\\n",
        "                    ## than the maximum info gain, store the greatest info gain among the keys\n",
        "                    if key_info_gain > max_info_gain:\n",
        "                        max_info_gain = key_info_gain\n",
        "                        ## The split attribute will be set to the key \\\n",
        "                        ## with the greatest information gain\n",
        "                        self.split_attribute = key\n",
        "\n",
        "                print(f\"Split by {self.split_attribute}, IG: {max_info_gain:.2f}\")\n",
        "\n",
        "                ## Create a dictionary for each unique children value\n",
        "                ## This dictionary will be used to refer each value to a specific branch of the tree\n",
        "                self.children = {}\n",
        "                for value in samples[self.split_attribute].unique():\n",
        "                    index = samples[self.split_attribute] == value\n",
        "                    ## Recursion, create and store new children nodes from split key\n",
        "                    self.children[value] = TreeNode()\n",
        "                    self.children[value].fit(samples[index], target[index])\n",
        "\n",
        "\n",
        "    def new_print(self, prefix=''):\n",
        "        if self.split_attribute is not None:\n",
        "            for k, v in self.children.items():\n",
        "                v.new_print(f\"{prefix}:When {self.split_attribute} is {k}\")\n",
        "        else:\n",
        "            print(f\"{prefix}:{self.decision}\")\n",
        "\n",
        "    def predict(self, sample):\n",
        "        if self.decision is not None:\n",
        "            # uncomment to get log information of code execution\n",
        "            # print(\"Decision:\", self.decision)\n",
        "            return self.decision\n",
        "        else: \n",
        "            # this node is an internal one, further queries about an attribute \n",
        "            # of the data is needed.\n",
        "            attr_val = sample[self.split_attribute]\n",
        "            child = self.children[attr_val]\n",
        "            return child.predict(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGvteBi__zrY",
        "outputId": "e0fc4b77-a044-4eb3-a64e-48e0b19a23c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Build the decision tree\n",
        "tree = TreeNode()\n",
        "\n",
        "# Assign \n",
        "sample_training_data = X_train_processed[attributes]\n",
        "target_training_data = y_train_processed[target]\n",
        "\n",
        "tree.fit(sample_training_data, target_training_data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split by ('New cases',), IG: 0.02\n",
            "Split by ('Active',), IG: 0.10\n",
            "Split by ('New recovered',), IG: 0.32\n",
            "Split by ('Confirmed',), IG: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEnJ1l9AgFxS",
        "outputId": "a7968c53-b487-44b7-e6b4-ff6267221fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Predict with testing dataset\n",
        "sample_testing_data = X_test_processed[attributes]\n",
        "target_testing_data = y_test_processed[target]\n",
        "\n",
        "for i in range(len(y_test_processed.columns)):\n",
        "    print(f\"Test predict sample [{sample_testing_data.columns[i][0]}]: \\n{sample_testing_data.iloc[i]}\\n\\tTarget: {y_test_processed.iloc[i]}\")\n",
        "    print(f\"Making decision ...\")\n",
        "\n",
        "    pred = tree.predict(sample_testing_data.iloc[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test predict sample [Confirmed]: \n",
            "Confirmed        0.0\n",
            "Deaths           0.0\n",
            "Recovered        0.0\n",
            "Active           0.0\n",
            "New cases        0.0\n",
            "New deaths       0.0\n",
            "New recovered    0.0\n",
            "Name: 0, dtype: float64\n",
            "\tTarget: Habitability    Yes\n",
            "Name: 0, dtype: object\n",
            "Making decision ...\n",
            "Decision: Yes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}